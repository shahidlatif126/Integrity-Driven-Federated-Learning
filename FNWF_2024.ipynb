{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1. Environment Setup:** Install necessary dependencies. We use pycryptodome for the secure Keccak-512 hashing algorithm and GPUtil for monitoring GPU resources."
      ],
      "metadata": {
        "id": "L3P0DBeBwXjW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries for Hashing and GPU monitoring\n",
        "!pip install GPUtil\n",
        "!pip install pycryptodome"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f3hSiauwxXh",
        "outputId": "4b1a9cb6-675d-42b7-8910-e2e46923ae7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting GPUtil\n",
            "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: GPUtil\n",
            "  Building wheel for GPUtil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7392 sha256=2b8e44bfa064c1c37a08112b65025259136ba713d4ee7f8157e503da15c0e357\n",
            "  Stored in directory: /root/.cache/pip/wheels/92/a8/b7/d8a067c31a74de9ca252bbe53dea5f896faabd25d55f541037\n",
            "Successfully built GPUtil\n",
            "Installing collected packages: GPUtil\n",
            "Successfully installed GPUtil-1.4.0\n",
            "Collecting pycryptodome\n",
            "  Downloading pycryptodome-3.23.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pycryptodome-3.23.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m84.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pycryptodome\n",
            "Successfully installed pycryptodome-3.23.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Imports & Configuration:** Import standard libraries for deep learning (TensorFlow/Keras), data manipulation, and cryptography."
      ],
      "metadata": {
        "id": "yU-SUC8Vw0vN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import pickle\n",
        "import psutil\n",
        "import subprocess\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import (\n",
        "    classification_report, accuracy_score, precision_score,\n",
        "    recall_score, f1_score, confusion_matrix\n",
        ")\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\n",
        "from Crypto.Hash import keccak\n",
        "\n",
        "# ==========================================\n",
        "# 1. CONFIGURATION & HYPERPARAMETERS\n",
        "# ==========================================\n",
        "\n",
        "# Federated Learning Settings\n",
        "TOTAL_CLIENTS = 10\n",
        "ROUNDS = 10\n",
        "EPOCHS = 5\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Attack Configuration\n",
        "# Note: In a 10-client simulation, we inject 1 of each attacker type.\n",
        "DATA_POISONING_CLIENTS = 1   # Attacks by flipping labels (High Loss)\n",
        "MODEL_POISONING_CLIENTS = 1  # Attacks by adding noise to weights (High Weight Divergence)\n",
        "TAMPERING_CLIENTS = 1        # Valid clients whose updates are corrupted in transit (Integrity Fail)\n",
        "\n",
        "# Detection Thresholds\n",
        "# Statistical multipliers for outlier detection (e.g., 2.0x the median)\n",
        "LOSS_MULTIPLIER = 2.0\n",
        "WEIGHT_MULTIPLIER = 2.0\n",
        "\n",
        "# Dataset Configuration\n",
        "# UPDATE THIS PATH to your local or Drive file location\n",
        "DATASET_PATH = '/content/drive/MyDrive/IDS-IOT2024/Process_1 IDS-IoT-2024.csv'\n",
        "# DATASET_PATH = 'Process_1 IDS-IoT-2024.csv'\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "print(\"Configuration complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3FTJSqlw-ZW",
        "outputId": "e0234136-ba02-4ee5-b02a-8849477f5a57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Utility Functions (Metrics & Hashing):** Define helper functions to monitor system resources (CPU/GPU memory), compute model size, and perform the core cryptographic operations: hash_weights (Keccak-512) and verify_hash."
      ],
      "metadata": {
        "id": "26eoC0-cxB58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_sys_memory():\n",
        "    \"\"\"Returns current process RAM usage in GB.\"\"\"\n",
        "    process = psutil.Process()\n",
        "    return process.memory_info().rss / (1024 ** 3)\n",
        "\n",
        "def get_gpu_memory():\n",
        "    \"\"\"Returns GPU memory usage in GB via nvidia-smi.\"\"\"\n",
        "    try:\n",
        "        result = subprocess.check_output(\n",
        "            ['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader']\n",
        "        )\n",
        "        return float(result.decode('utf-8').strip()) / 1024\n",
        "    except:\n",
        "        return 0.0\n",
        "\n",
        "def hash_weights(weights):\n",
        "    \"\"\"\n",
        "    Computes the Keccak-512 hash of the model weights.\n",
        "    Returns: Hex digest string.\n",
        "    \"\"\"\n",
        "    weights_serialized = pickle.dumps(weights)\n",
        "    hash_obj = keccak.new(digest_bits=512)\n",
        "    hash_obj.update(weights_serialized)\n",
        "    return hash_obj.hexdigest()\n",
        "\n",
        "def verify_hash(weights, received_hash):\n",
        "    \"\"\"\n",
        "    Verifies if the computed hash of 'weights' matches 'received_hash'.\n",
        "    Returns: Boolean (True if valid).\n",
        "    \"\"\"\n",
        "    return hash_weights(weights) == received_hash\n",
        "\n",
        "def get_model_size_kb(model):\n",
        "    \"\"\"Calculates model size in Kilobytes (KB).\"\"\"\n",
        "    # Assuming float32 (4 bytes per parameter)\n",
        "    bytes_size = sum(np.prod(w.shape) for w in model.get_weights()) * 4\n",
        "    return bytes_size / 1024\n",
        "\n",
        "def get_communication_size_mb(model):\n",
        "    \"\"\"Calculates model size in Megabytes (MB) for communication overhead.\"\"\"\n",
        "    return get_model_size_kb(model) / 1024"
      ],
      "metadata": {
        "id": "ylgau2nLxLhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Data Loading & Preprocessing:** Load the IDSIoT2024 dataset, encode labels, and split the data into training (for clients) and testing (for server evaluation) sets."
      ],
      "metadata": {
        "id": "6HM4wvf-xO9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_and_preprocess_data():\n",
        "    print(f\"[*] Loading dataset: {DATASET_PATH}\")\n",
        "    try:\n",
        "        df = pd.read_csv(DATASET_PATH)\n",
        "        print(f\"    - Shape: {df.shape}\")\n",
        "\n",
        "        X = df.iloc[:, :-1].values\n",
        "        y = df['Attack_Category_x'].values\n",
        "\n",
        "        # Encode Labels\n",
        "        encoder = LabelEncoder()\n",
        "        y_encoded = encoder.fit_transform(y)\n",
        "        num_classes = len(np.unique(y_encoded))\n",
        "\n",
        "        # Split Data (80% Global Train (distributed to clients), 20% Global Test)\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y_encoded, test_size=0.2, random_state=SEED\n",
        "        )\n",
        "        return X_train, X_test, y_train, y_test, num_classes, encoder\n",
        "    except FileNotFoundError:\n",
        "        print(f\"[!] Error: File not found at {DATASET_PATH}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "DK4KwqFYxXqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Model Architecture & Client Setup:** Define the lightweight Feed-Forward Neural Network (FFNN) architecture. We also create the distribute_data_and_assign_roles function to partition data among clients and assign specific attack roles (Data Poisoning, Model Poisoning, Tampering)."
      ],
      "metadata": {
        "id": "9ul0WRnjxfaI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_lightweight_model(input_dim, num_classes):\n",
        "    \"\"\"\n",
        "    Defines the Lightweight FFNN architecture from the paper.\n",
        "    Structure: Input -> Dense(128) -> BN -> Dropout -> Dense(64) -> BN -> Dropout -> Output\n",
        "    \"\"\"\n",
        "    model = Sequential([\n",
        "        Input(shape=(input_dim,)),\n",
        "        Dense(128, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.2),\n",
        "        Dense(64, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.2),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='AdamW', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def distribute_data_and_assign_roles(X, y):\n",
        "    \"\"\"\n",
        "    Splits data among clients and assigns attack roles.\n",
        "    Returns: client_data list and sets of attacker indices.\n",
        "    \"\"\"\n",
        "    client_data = []\n",
        "    indices = list(range(TOTAL_CLIENTS))\n",
        "\n",
        "    # Assign Roles\n",
        "    malicious = random.sample(indices, DATA_POISONING_CLIENTS + MODEL_POISONING_CLIENTS)\n",
        "    data_poisoners = set(malicious[:DATA_POISONING_CLIENTS])\n",
        "    model_poisoners = set(malicious[DATA_POISONING_CLIENTS:])\n",
        "\n",
        "    valid_pool = [i for i in indices if i not in malicious]\n",
        "    tamper_victims = set(random.sample(valid_pool, TAMPERING_CLIENTS))\n",
        "\n",
        "    print(f\"[*] Role Assignment:\")\n",
        "    print(f\"    - Data Poisoners: {data_poisoners}\")\n",
        "    print(f\"    - Model Poisoners: {model_poisoners}\")\n",
        "    print(f\"    - Tampering Victims: {tamper_victims}\")\n",
        "\n",
        "    # Distribute Data\n",
        "    for i in indices:\n",
        "        # IID Split for simplicity\n",
        "        X_c, _, y_c, _ = train_test_split(X, y, test_size=0.9, random_state=i) # Small subset per client\n",
        "\n",
        "        # Apply Data Poisoning (Label Flipping) immediately if applicable\n",
        "        if i in data_poisoners:\n",
        "            y_c = np.roll(y_c, shift=1, axis=0) # Shift labels cyclically\n",
        "\n",
        "        client_data.append((X_c, y_c))\n",
        "\n",
        "    return client_data, data_poisoners, model_poisoners, tamper_victims"
      ],
      "metadata": {
        "id": "0bOkcT-8xnBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Server Detection Logic:** Implement the statistical detection mechanism (The Poisoning Gate). This function analyzes client losses and weight updates to identify outliers, flagging them as potential Data or Model poisoning attacks."
      ],
      "metadata": {
        "id": "0pPFI9S6xrgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_poisoning(client_losses, weight_diffs, round_num):\n",
        "    \"\"\"\n",
        "    Statistical Outlier Detection.\n",
        "    Identifies clients whose Loss or Weight Updates deviate significantly from the median.\n",
        "    \"\"\"\n",
        "    detected = []\n",
        "\n",
        "    # Calculate medians (Robust to outliers)\n",
        "    med_loss = np.median(client_losses) if client_losses else 0\n",
        "    med_diff = np.median([np.mean(w) for w in weight_diffs]) if weight_diffs else 0\n",
        "\n",
        "    # Dynamic thresholding: We ignore detection in Round 1 (Warm-up)\n",
        "    if round_num == 0:\n",
        "        return []\n",
        "\n",
        "    for i, (loss, diff_list) in enumerate(zip(client_losses, weight_diffs)):\n",
        "        avg_diff = np.mean(diff_list)\n",
        "\n",
        "        # 1. Data Poisoning Check (Loss >> Median)\n",
        "        if loss > (med_loss * LOSS_MULTIPLIER) and loss > 0.5: # 0.5 is a safe floor\n",
        "            detected.append((i, 'Data Poisoning'))\n",
        "\n",
        "        # 2. Model Poisoning Check (Update Dist >> Median)\n",
        "        elif avg_diff > (med_diff * WEIGHT_MULTIPLIER) and avg_diff > 0.1:\n",
        "            detected.append((i, 'Model Poisoning'))\n",
        "\n",
        "    return detected"
      ],
      "metadata": {
        "id": "hU3eqUJAxx4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. Main Training Simulation:** The core simulation loop. It orchestrates the entire Integrity-Driven Federated Learning process:\n",
        "\n",
        "\n",
        "1.   Client Training: Local training, hash computation, and attack simulation (MITM/Tampering).\n",
        "2.   Server Verification: Integrity check (Gate 1) -> Poisoning detection (Gate 2) -> Aggregation.\n",
        "3.   Computing global accuracy, throughput, overheads, and resource usage.\n"
      ],
      "metadata": {
        "id": "pOUyL0LEx4Mv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Init Data\n",
        "data_pack = load_and_preprocess_data()\n",
        "if not data_pack:\n",
        "    print(\"Simulation stopped due to missing data.\")\n",
        "else:\n",
        "    X_train, X_test, y_train, y_test, n_classes, encoder = data_pack\n",
        "\n",
        "    # 2. Init Clients & Roles\n",
        "    client_data, _, model_poisoners, tamper_victims = distribute_data_and_assign_roles(X_train, y_train)\n",
        "\n",
        "    # 3. Init Models\n",
        "    # Global Model (Server)\n",
        "    global_model = create_lightweight_model(X_train.shape[1], n_classes)\n",
        "    # Reusable Client Model (Optimization: Prevents memory leaks/freezing)\n",
        "    client_worker = create_lightweight_model(X_train.shape[1], n_classes)\n",
        "\n",
        "    model_size_mb = get_communication_size_mb(global_model)\n",
        "    model_size_kb = get_model_size_kb(global_model)\n",
        "\n",
        "    # 4. Metric Logs\n",
        "    history = {\n",
        "        'acc': [], 'f1': [], 'prec': [], 'rec': [],\n",
        "        'throughput_samples': [], 'throughput_mbs': [],\n",
        "        'comp_time': [], 'comm_overhead': [],\n",
        "        'hash_comp': [], 'hash_verif': [],\n",
        "        'cpu': [], 'gpu': []\n",
        "    }\n",
        "\n",
        "    excluded_clients = set()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"STARTING FEDERATED LEARNING SIMULATION\")\n",
        "    print(f\"Clients: {TOTAL_CLIENTS} | Rounds: {ROUNDS}\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    for r in range(ROUNDS):\n",
        "        round_start = time.time()\n",
        "        print(f\"\\n--- Round {r+1}/{ROUNDS} ---\")\n",
        "\n",
        "        # Buffers for server-side processing\n",
        "        updates_buffer = []\n",
        "\n",
        "        # --- CLIENT PHASE ---\n",
        "        for cid, (X_c, y_c) in enumerate(client_data):\n",
        "            if cid in excluded_clients:\n",
        "                continue\n",
        "\n",
        "            # 1. Download Global Weights\n",
        "            client_worker.set_weights(global_model.get_weights())\n",
        "\n",
        "            # 2. Local Training (Measure Computation Overhead)\n",
        "            t_start_train = time.time()\n",
        "            hist = client_worker.fit(X_c, y_c, epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=0)\n",
        "            t_train_dur = time.time() - t_start_train\n",
        "\n",
        "            # Get Parameters\n",
        "            weights = client_worker.get_weights()\n",
        "            loss = hist.history['loss'][-1]\n",
        "\n",
        "            # Apply Model Poisoning (Noise Injection) if applicable\n",
        "            if cid in model_poisoners:\n",
        "                weights = [w + np.random.normal(0, 0.5, w.shape) for w in weights]\n",
        "\n",
        "            # 3. Hash Computation (Measure Time)\n",
        "            t_start_hash = time.time()\n",
        "            c_hash = hash_weights(weights)\n",
        "            t_hash_dur = time.time() - t_start_hash\n",
        "\n",
        "            # 4. Transmission Simulation (MITM Tampering)\n",
        "            tx_weights = weights\n",
        "            if cid in tamper_victims:\n",
        "                # Modify weights in transit (Hash will NOT match)\n",
        "                tx_weights = [w + np.random.normal(0, 0.01, w.shape) for w in weights]\n",
        "\n",
        "            updates_buffer.append({\n",
        "                'id': cid,\n",
        "                'weights': tx_weights,\n",
        "                'hash': c_hash,\n",
        "                'loss': loss,\n",
        "                'comp_overhead': t_train_dur, # Paper defines computation overhead as training time\n",
        "                'hash_overhead': t_hash_dur\n",
        "            })\n",
        "\n",
        "            # Simple progress indicator\n",
        "            print(\".\", end=\"\", flush=True)\n",
        "\n",
        "        print(\" [Upload Complete]\")\n",
        "\n",
        "        # --- SERVER PHASE ---\n",
        "\n",
        "        valid_updates = []\n",
        "        round_losses = []\n",
        "        round_diffs = []\n",
        "        temp_map = [] # Maps index back to updates_buffer\n",
        "\n",
        "        hash_verif_times = []\n",
        "\n",
        "        # Gate 1: Integrity Verification\n",
        "        for update in updates_buffer:\n",
        "            t_v_start = time.time()\n",
        "            is_valid = verify_hash(update['weights'], update['hash'])\n",
        "            hash_verif_times.append(time.time() - t_v_start)\n",
        "\n",
        "            if not is_valid:\n",
        "                # Dropped silently or logged\n",
        "                # print(f\"    [!] Integrity Check Failed: Client {update['id']}\")\n",
        "                continue\n",
        "\n",
        "            # Prepare for Poisoning Check\n",
        "            # Calculate L2 norm distance from global model\n",
        "            diff = [np.linalg.norm(update['weights'][i] - global_model.get_weights()[i])\n",
        "                    for i in range(len(update['weights']))]\n",
        "\n",
        "            round_losses.append(update['loss'])\n",
        "            round_diffs.append(diff)\n",
        "            temp_map.append(update)\n",
        "\n",
        "        # Gate 2: Poisoning Detection\n",
        "        detected_attacks = detect_poisoning(round_losses, round_diffs, r)\n",
        "        poison_indices = [x[0] for x in detected_attacks]\n",
        "\n",
        "        # Log and Exclude\n",
        "        for local_idx, reason in detected_attacks:\n",
        "            real_id = temp_map[local_idx]['id']\n",
        "            if real_id not in excluded_clients:\n",
        "                print(f\"    [!] DETECTED Client {real_id}: {reason}. Banning.\")\n",
        "                excluded_clients.add(real_id)\n",
        "\n",
        "        # Gate 3: Aggregation\n",
        "        for i, update in enumerate(temp_map):\n",
        "            if i not in poison_indices:\n",
        "                valid_updates.append(update['weights'])\n",
        "\n",
        "        if valid_updates:\n",
        "            # FedAvg\n",
        "            new_weights = [np.mean([w[i] for w in valid_updates], axis=0)\n",
        "                           for i in range(len(valid_updates[0]))]\n",
        "            global_model.set_weights(new_weights)\n",
        "            print(f\"    > Aggregated {len(valid_updates)} updates.\")\n",
        "        else:\n",
        "            print(\"    > No valid updates to aggregate.\")\n",
        "\n",
        "        # --- EVALUATION PHASE ---\n",
        "\n",
        "        # 1. Classification Metrics\n",
        "        y_pred = np.argmax(global_model.predict(X_test, verbose=0), axis=1)\n",
        "\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        prec = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "        rec = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "        f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "\n",
        "        # 2. Performance Metrics\n",
        "        r_time = time.time() - round_start\n",
        "        throughput_samples = len(X_train) / r_time # Samples processed per second\n",
        "\n",
        "        # Communication Overhead: Size of model * number of clients who sent updates\n",
        "        comm_overhead_mb = model_size_mb * len(updates_buffer)\n",
        "\n",
        "        # Throughput in Mbs (Megabits per second)\n",
        "        throughput_mbs = (comm_overhead_mb * 8) / r_time\n",
        "\n",
        "        # Avg Computation Overhead (Training time per client)\n",
        "        avg_comp_time = np.mean([u['comp_overhead'] for u in updates_buffer]) * 1000 # ms\n",
        "\n",
        "        # Hash Times (seconds)\n",
        "        avg_hash_comp_sec = np.mean([u['hash_overhead'] for u in updates_buffer])\n",
        "        avg_hash_verif_sec = np.mean(hash_verif_times) if hash_verif_times else 0\n",
        "\n",
        "        # Resources\n",
        "        cpu_usage = get_sys_memory()\n",
        "        gpu_usage = get_gpu_memory()\n",
        "\n",
        "        # Store\n",
        "        history['acc'].append(acc)\n",
        "        history['f1'].append(f1)\n",
        "        history['prec'].append(prec)\n",
        "        history['rec'].append(rec)\n",
        "        history['throughput_samples'].append(throughput_samples)\n",
        "        history['throughput_mbs'].append(throughput_mbs)\n",
        "        history['comp_time'].append(avg_comp_time)\n",
        "        history['comm_overhead'].append(comm_overhead_mb)\n",
        "        history['hash_comp'].append(avg_hash_comp_sec)\n",
        "        history['hash_verif'].append(avg_hash_verif_sec)\n",
        "        history['cpu'].append(cpu_usage)\n",
        "        history['gpu'].append(gpu_usage)\n",
        "\n",
        "        # Print Round Summary\n",
        "        print(f\"    Accuracy: {acc:.4f} | F1: {f1:.4f}\")\n",
        "        print(f\"    Throughput: {throughput_samples:.2f} samples/sec | {throughput_mbs:.4f} Mbps\")\n",
        "        print(f\"    Avg Comp Overhead: {avg_comp_time:.2f} ms\")\n",
        "        print(f\"    Avg Hash Gen Time: {avg_hash_comp_sec:.6f} s | Verif Time: {avg_hash_verif_sec:.6f} s\")\n",
        "        print(f\"    CPU Usage: {cpu_usage:.2f} GB | GPU Usage: {gpu_usage:.4f} GB\")"
      ],
      "metadata": {
        "id": "lyj1V-lVyLE_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}